{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Few Shot Learning NLP Library Documentation This library provides tools and utilities for Few Shot Learning in Natural Language Processing (NLP). Installation You can install this library via pip: pip install -U few-shot-learning-nlp Overview Few Shot Learning in NLP involves training and evaluating models on tasks with limited labeled data. This library offers functionalities to facilitate this process. Supported Approachs Text Classification Sentence Transformers Finetuning ( SetFit ) Pattern Exploiting Training ( PET ) Named Entity Recognition for Image Documents Pattern Exploiting Training ( PET ) Bio Technique Classification utils Focal Loss function for imbalanced datasets Stratified train test split Usage To utilize this library, import the necessary classes and methods and follow the provided documentation for each component.","title":"Home"},{"location":"#welcome-to-the-few-shot-learning-nlp-library-documentation","text":"This library provides tools and utilities for Few Shot Learning in Natural Language Processing (NLP).","title":"Welcome to the Few Shot Learning NLP Library Documentation"},{"location":"#installation","text":"You can install this library via pip: pip install -U few-shot-learning-nlp","title":"Installation"},{"location":"#overview","text":"Few Shot Learning in NLP involves training and evaluating models on tasks with limited labeled data. This library offers functionalities to facilitate this process.","title":"Overview"},{"location":"#supported-approachs","text":"","title":"Supported Approachs"},{"location":"#text-classification","text":"Sentence Transformers Finetuning ( SetFit ) Pattern Exploiting Training ( PET )","title":"Text Classification"},{"location":"#named-entity-recognition-for-image-documents","text":"Pattern Exploiting Training ( PET ) Bio Technique","title":"Named Entity Recognition for Image Documents"},{"location":"#classification-utils","text":"Focal Loss function for imbalanced datasets Stratified train test split","title":"Classification utils"},{"location":"#usage","text":"To utilize this library, import the necessary classes and methods and follow the provided documentation for each component.","title":"Usage"},{"location":"bio/","text":"Bio Technique for Named Entity Recognition on Image Documents Introduction The BioTechnique approach adapts multi-class classification into a binary framework by predicting the class membership of each token in a text. It achieves this by appending each token with its possible class label, thereby increasing the data volume by a factor of k, where k represents the number of classes. This augmentation enables binary classification for each token, facilitating more granular classification within the document. In the BioTechnique approach, suppose we have the classes C = [City, Transport, None]. For a given sentence \"Paris has a good metro system\", we transform it into three separate sentences: City [SEP] Paris has a good metro system Transport [SEP] Paris has a good metro system None [SEP] Paris has a good metro system Each sentence is treated as a binary classification task to predict whether each token belongs to its corresponding class or not. This approach effectively increases the data volume and allows for more precise classification within the document. Usage example Imports %load_ext autoreload %autoreload 2 import transformers from transformers import AutoTokenizer, AutoModelForTokenClassification from datasets import load_dataset, Dataset import torch from torch.utils.data import DataLoader import matplotlib.pyplot as plt import numpy as np from torcheval.metrics.functional import multiclass_f1_score, multiclass_confusion_matrix, binary_f1_score from few_shot_learning_nlp.few_shot_ner_image_documents.bio_technique import BioTrainer from few_shot_learning_nlp.few_shot_ner_image_documents.image_dataset import ImageLayoutDataset from few_shot_learning_nlp.few_shot_ner_image_documents.bio_technique_dataset import generate_dataset device = \"cuda\" if torch.cuda.is_available() else \"cpu\" Pre-processing The dataset is loaded from the FUNSD dataset, and necessary pre-processing steps are performed, including tokenization and dataset generation. Training # Loading FUNSD Dataset funsd_dataset = load_dataset(\"nielsr/funsd\") tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # Generating dataset train_data = generate_dataset(funsd_dataset['train'], label_names, idx2label, tokenizer, n_shots=2) val_data = generate_dataset(Dataset.from_dict(funsd_dataset['train'][10:]), label_names, idx2label, tokenizer, n_shots=50) test_data = generate_dataset(funsd_dataset['test'], label_names, idx2label, tokenizer, n_shots=np.inf) # Creating DataLoader train_dataset = ImageLayoutDataset(train_data, tokenizer) train_dataloader = DataLoader(train_dataset, shuffle=False) validation_dataset = ImageLayoutDataset(val_data, tokenizer) validation_dataloader = DataLoader(validation_dataset, shuffle=False, batch_size=4) test_dataset = ImageLayoutDataset(test_data, tokenizer) test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=4) # Initializing and training the model model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=2) model.to(device) optimizer = torch.optim.Adam(model.parameters(), lr=1e-5) trainer = BioTrainer(model, optimizer, n_classes) history = trainer.train(train_dataloader, validation_dataloader, n_epochs=100) # Plotting validation performance best_f1, best_epoch = np.max(history), np.argmax(history) plt.plot(history) plt.scatter([best_epoch], [best_f1], color=\"green\", label=f\"Best f1 : {round(best_f1,3)}\") plt.legend() plt.ylabel(\"f1 score\") plt.xlabel(\"epoch\") plt.title(\"Validation performance - FUNSD - 2 shots\") plt.grid() Evaluation The model's performance is evaluated using the test dataset, and the F1 score and confusion matrix are computed. # Evaluating on the test set y_true, y_pred = trainer.test(test_dataloader) f1 = multiclass_f1_score(y_pred.to(torch.int64), y_true.to(torch.int64), num_classes=n_classes) This notebook showcases the implementation of Few-Shot Learning for NER on image documents using transformers and PyTorch. title: BioTrainer Class Documentation BioTrainer Methods __init__(model, optimizer, n_classes, device=\"cuda\") Initialize BioTrainer with the provided token classification model, optimizer, and other parameters. model (AutoModelForTokenClassification) : The token classification model to be trained. optimizer (torch.optim) : The optimizer used for training. n_classes (int) : The number of classes for token classification. device (str, optional) : The device where the model will be trained. Defaults to \"cuda\". train(train_dataloader, validation_dataloader, n_epochs=20) Train and validate the token classification model. train_dataloader (Dataset) : DataLoader containing the training data. Requires batch size of 1. validation_dataloader (Dataset) : DataLoader containing the validation data. Requires batch size of 1. n_epochs (int, optional) : Number of epochs for training. Defaults to 20. Returns: - history (list) : History of evaluation metric (F1-score) during training. test(test_dataloader) Performs testing on the provided test dataloader. test_dataloader (DataLoader) : The dataloader containing the test dataset. Requires batch size of 1. Returns: - Tuple[torch.Tensor, torch.Tensor] : A tuple containing the true labels and predicted labels. Attributes history (list) : List to store the evaluation metric (F1-score) history during training. best_model (AutoModelForTokenClassification) : The best-performing model based on validation F1-score. n_classes (int) : The number of classes for token classification. model (AutoModelForTokenClassification) : The token classification model. optimizer (torch.optim) : The optimizer used for training. device (str) : The device where the model will be trained. generate_dataset Function Generates a new dataset by modifying the original dataset based on the given parameters. Arguments dataset (Dataset) : The original dataset. label_names (List[str]) : A list of label names to generate the dataset. idx2label (Dict[int, str]) : A dictionary mapping label indices to label names. tokenizer (AutoTokenizer) : The tokenizer used to tokenize words. n_shots (int) : The number of shots to consider from the original dataset. Returns Dataset : The generated dataset. Notes The function generates a new dataset by modifying the original dataset. It creates additional samples based on the provided label names and the number of shots specified. Each document in the original dataset is processed to generate new samples. Only a limited number of shots ( n_shots ) are considered from the original dataset. For each label in label_names , the function creates new samples where the tokens belonging to that label are marked with 1 and other tokens with -100 in the ner_tags field. The words field in the new dataset contains the tokenized words, and the bboxes field contains the corresponding bounding boxes.","title":"Bio Technique"},{"location":"bio/#bio-technique-for-named-entity-recognition-on-image-documents","text":"","title":"Bio Technique for Named Entity Recognition on Image Documents"},{"location":"bio/#introduction","text":"The BioTechnique approach adapts multi-class classification into a binary framework by predicting the class membership of each token in a text. It achieves this by appending each token with its possible class label, thereby increasing the data volume by a factor of k, where k represents the number of classes. This augmentation enables binary classification for each token, facilitating more granular classification within the document. In the BioTechnique approach, suppose we have the classes C = [City, Transport, None]. For a given sentence \"Paris has a good metro system\", we transform it into three separate sentences: City [SEP] Paris has a good metro system Transport [SEP] Paris has a good metro system None [SEP] Paris has a good metro system Each sentence is treated as a binary classification task to predict whether each token belongs to its corresponding class or not. This approach effectively increases the data volume and allows for more precise classification within the document.","title":"Introduction"},{"location":"bio/#usage-example","text":"","title":"Usage example"},{"location":"bio/#imports","text":"%load_ext autoreload %autoreload 2 import transformers from transformers import AutoTokenizer, AutoModelForTokenClassification from datasets import load_dataset, Dataset import torch from torch.utils.data import DataLoader import matplotlib.pyplot as plt import numpy as np from torcheval.metrics.functional import multiclass_f1_score, multiclass_confusion_matrix, binary_f1_score from few_shot_learning_nlp.few_shot_ner_image_documents.bio_technique import BioTrainer from few_shot_learning_nlp.few_shot_ner_image_documents.image_dataset import ImageLayoutDataset from few_shot_learning_nlp.few_shot_ner_image_documents.bio_technique_dataset import generate_dataset device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","title":"Imports"},{"location":"bio/#pre-processing","text":"The dataset is loaded from the FUNSD dataset, and necessary pre-processing steps are performed, including tokenization and dataset generation.","title":"Pre-processing"},{"location":"bio/#training","text":"# Loading FUNSD Dataset funsd_dataset = load_dataset(\"nielsr/funsd\") tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # Generating dataset train_data = generate_dataset(funsd_dataset['train'], label_names, idx2label, tokenizer, n_shots=2) val_data = generate_dataset(Dataset.from_dict(funsd_dataset['train'][10:]), label_names, idx2label, tokenizer, n_shots=50) test_data = generate_dataset(funsd_dataset['test'], label_names, idx2label, tokenizer, n_shots=np.inf) # Creating DataLoader train_dataset = ImageLayoutDataset(train_data, tokenizer) train_dataloader = DataLoader(train_dataset, shuffle=False) validation_dataset = ImageLayoutDataset(val_data, tokenizer) validation_dataloader = DataLoader(validation_dataset, shuffle=False, batch_size=4) test_dataset = ImageLayoutDataset(test_data, tokenizer) test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=4) # Initializing and training the model model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=2) model.to(device) optimizer = torch.optim.Adam(model.parameters(), lr=1e-5) trainer = BioTrainer(model, optimizer, n_classes) history = trainer.train(train_dataloader, validation_dataloader, n_epochs=100) # Plotting validation performance best_f1, best_epoch = np.max(history), np.argmax(history) plt.plot(history) plt.scatter([best_epoch], [best_f1], color=\"green\", label=f\"Best f1 : {round(best_f1,3)}\") plt.legend() plt.ylabel(\"f1 score\") plt.xlabel(\"epoch\") plt.title(\"Validation performance - FUNSD - 2 shots\") plt.grid()","title":"Training"},{"location":"bio/#evaluation","text":"The model's performance is evaluated using the test dataset, and the F1 score and confusion matrix are computed. # Evaluating on the test set y_true, y_pred = trainer.test(test_dataloader) f1 = multiclass_f1_score(y_pred.to(torch.int64), y_true.to(torch.int64), num_classes=n_classes) This notebook showcases the implementation of Few-Shot Learning for NER on image documents using transformers and PyTorch.","title":"Evaluation"},{"location":"bio/#title-biotrainer-class-documentation","text":"","title":"title: BioTrainer Class Documentation"},{"location":"bio/#biotrainer","text":"","title":"BioTrainer"},{"location":"bio/#methods","text":"__init__(model, optimizer, n_classes, device=\"cuda\") Initialize BioTrainer with the provided token classification model, optimizer, and other parameters. model (AutoModelForTokenClassification) : The token classification model to be trained. optimizer (torch.optim) : The optimizer used for training. n_classes (int) : The number of classes for token classification. device (str, optional) : The device where the model will be trained. Defaults to \"cuda\". train(train_dataloader, validation_dataloader, n_epochs=20) Train and validate the token classification model. train_dataloader (Dataset) : DataLoader containing the training data. Requires batch size of 1. validation_dataloader (Dataset) : DataLoader containing the validation data. Requires batch size of 1. n_epochs (int, optional) : Number of epochs for training. Defaults to 20. Returns: - history (list) : History of evaluation metric (F1-score) during training. test(test_dataloader) Performs testing on the provided test dataloader. test_dataloader (DataLoader) : The dataloader containing the test dataset. Requires batch size of 1. Returns: - Tuple[torch.Tensor, torch.Tensor] : A tuple containing the true labels and predicted labels.","title":"Methods"},{"location":"bio/#attributes","text":"history (list) : List to store the evaluation metric (F1-score) history during training. best_model (AutoModelForTokenClassification) : The best-performing model based on validation F1-score. n_classes (int) : The number of classes for token classification. model (AutoModelForTokenClassification) : The token classification model. optimizer (torch.optim) : The optimizer used for training. device (str) : The device where the model will be trained.","title":"Attributes"},{"location":"bio/#generate_dataset-function","text":"Generates a new dataset by modifying the original dataset based on the given parameters.","title":"generate_dataset Function"},{"location":"bio/#arguments","text":"dataset (Dataset) : The original dataset. label_names (List[str]) : A list of label names to generate the dataset. idx2label (Dict[int, str]) : A dictionary mapping label indices to label names. tokenizer (AutoTokenizer) : The tokenizer used to tokenize words. n_shots (int) : The number of shots to consider from the original dataset.","title":"Arguments"},{"location":"bio/#returns","text":"Dataset : The generated dataset.","title":"Returns"},{"location":"bio/#notes","text":"The function generates a new dataset by modifying the original dataset. It creates additional samples based on the provided label names and the number of shots specified. Each document in the original dataset is processed to generate new samples. Only a limited number of shots ( n_shots ) are considered from the original dataset. For each label in label_names , the function creates new samples where the tokens belonging to that label are marked with 1 and other tokens with -100 in the ner_tags field. The words field in the new dataset contains the tokenized words, and the bboxes field contains the corresponding bounding boxes.","title":"Notes"},{"location":"images/","text":"ImageLayoutDataset A PyTorch Dataset for handling image layout data with tokenization and labeling. Arguments data (List[Dict]) : A list of dictionaries containing the image layout data. Each dictionary should contain at least the following keys: 'words' : List of words in the text. 'bboxes' : List of bounding boxes corresponding to each word. 'ner_tags' : List of named entity recognition tags. tokenizer : The tokenizer to tokenize the text. device (str, optional) : The device where tensors will be placed. Defaults to 'cuda'. encode (bool, optional) : Whether to encode the data during initialization. Defaults to True. tokenize_all_labels (bool, optional) : Whether to tokenize all labels or only the first token of a word. Defaults to False. valid_labels_keymap (Dict, optional) : A dictionary mapping valid labels to their corresponding token ids. Defaults to None. Methods tokenize_labels(ner_tags, tokens) : Tokenizes and aligns the labels with the tokens. tokenize_boxes(words, boxes) : Tokenizes the bounding boxes and pads them to match the sequence length. encode(example) : Encodes an example from the dataset. __getitem__(index) : Retrieves an item from the dataset at the specified index. __len__() : Returns the length of the dataset. Attributes tokenizer : The tokenizer used for tokenization. device (str) : The device where tensors will be placed. valid_labels_keymap (Dict) : A dictionary mapping valid labels to their corresponding token ids. tokenize_all_labels (bool) : Whether to tokenize all labels or only the first token of a word. X (List) : List to store the encoded data or raw data. Usage Example from transformers import AutoTokenizer from torch.utils.data import DataLoader from datasets import load_dataset from few_shot_learning_nlp.few_shot_ner_image_documents.image_dataset import ImageLayoutDataset # Load the FUNSD dataset dataset = load_dataset(\"nielsr/funsd\") # Example tokenizer tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # Example data from the FUNSD dataset data = dataset[\"train\"] # Initialize the dataset image_layout_dataset = ImageLayoutDataset(data, tokenizer) # Get the length of the dataset print(\"Dataset length:\", len(image_layout_dataset)) # Get an item from the dataset example_item = image_layout_dataset[0] print(\"Example item:\", example_item) # DataLoader example loader = DataLoader(image_layout_dataset, batch_size=4, shuffle=True) for batch in loader: print(\"Batch shape:\", batch.shape)","title":"Image documents transformer"},{"location":"images/#imagelayoutdataset","text":"A PyTorch Dataset for handling image layout data with tokenization and labeling.","title":"ImageLayoutDataset"},{"location":"images/#arguments","text":"data (List[Dict]) : A list of dictionaries containing the image layout data. Each dictionary should contain at least the following keys: 'words' : List of words in the text. 'bboxes' : List of bounding boxes corresponding to each word. 'ner_tags' : List of named entity recognition tags. tokenizer : The tokenizer to tokenize the text. device (str, optional) : The device where tensors will be placed. Defaults to 'cuda'. encode (bool, optional) : Whether to encode the data during initialization. Defaults to True. tokenize_all_labels (bool, optional) : Whether to tokenize all labels or only the first token of a word. Defaults to False. valid_labels_keymap (Dict, optional) : A dictionary mapping valid labels to their corresponding token ids. Defaults to None.","title":"Arguments"},{"location":"images/#methods","text":"tokenize_labels(ner_tags, tokens) : Tokenizes and aligns the labels with the tokens. tokenize_boxes(words, boxes) : Tokenizes the bounding boxes and pads them to match the sequence length. encode(example) : Encodes an example from the dataset. __getitem__(index) : Retrieves an item from the dataset at the specified index. __len__() : Returns the length of the dataset.","title":"Methods"},{"location":"images/#attributes","text":"tokenizer : The tokenizer used for tokenization. device (str) : The device where tensors will be placed. valid_labels_keymap (Dict) : A dictionary mapping valid labels to their corresponding token ids. tokenize_all_labels (bool) : Whether to tokenize all labels or only the first token of a word. X (List) : List to store the encoded data or raw data.","title":"Attributes"},{"location":"images/#usage-example","text":"from transformers import AutoTokenizer from torch.utils.data import DataLoader from datasets import load_dataset from few_shot_learning_nlp.few_shot_ner_image_documents.image_dataset import ImageLayoutDataset # Load the FUNSD dataset dataset = load_dataset(\"nielsr/funsd\") # Example tokenizer tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # Example data from the FUNSD dataset data = dataset[\"train\"] # Initialize the dataset image_layout_dataset = ImageLayoutDataset(data, tokenizer) # Get the length of the dataset print(\"Dataset length:\", len(image_layout_dataset)) # Get an item from the dataset example_item = image_layout_dataset[0] print(\"Example item:\", example_item) # DataLoader example loader = DataLoader(image_layout_dataset, batch_size=4, shuffle=True) for batch in loader: print(\"Batch shape:\", batch.shape)","title":"Usage Example"},{"location":"loss/","text":"FocalLoss Introduction The FocalLoss class implements the Focal Loss criterion, which is used to address class imbalance in classification tasks. It is a modification of the standard cross-entropy loss that down-weights well-classified examples and focuses on hard-to-classify examples. Usage example import torch import numpy as np from few_shot_learning_nlp.loss import FocalLoss # Assuming train_df is your training DataFrame _, class_counts = np.unique(train_df['label'], return_counts=True) # Calculate alpha alpha = len(train_df['label']) / class_counts # Initialize FocalLoss loss_fn = FocalLoss(alpha, gamma=2) Implementation Initialization def __init__( self, alpha: Union[list, float, int], gamma: int = 2, device: str = 'cuda' ): alpha : The weight factor(s) for each class to address class imbalance. If a single value is provided, it is assumed to be the weight for the positive class, and the weight for the negative class is calculated as 1 - alpha. If a list is provided, it should contain weight factors for each class. gamma : The focusing parameter to control the degree of adjustment for misclassified samples. Higher values of gamma give more weight to hard-to-classify examples, reducing the influence of easy examples (default: 2). device : The device on which to perform calculations ('cuda' or 'cpu') (default: 'cuda'). Attributes alpha (torch.Tensor): The calculated weight factors for each class. gamma (float): The focusing parameter. device (str): The device on which calculations are performed. Methods forward(inputs, targets) Compute the Focal Loss given the input predictions and target labels. Args: inputs (torch.Tensor): The input predictions or logits from the model. targets (torch.Tensor): The target class labels. Returns: torch.Tensor : The computed Focal Loss value. Note Ensure that the inputs are logits or unnormalized probabilities, and the targets are class labels. This implementation supports binary or multiclass classification tasks.","title":"Focal Loss"},{"location":"loss/#focalloss","text":"","title":"FocalLoss"},{"location":"loss/#introduction","text":"The FocalLoss class implements the Focal Loss criterion, which is used to address class imbalance in classification tasks. It is a modification of the standard cross-entropy loss that down-weights well-classified examples and focuses on hard-to-classify examples.","title":"Introduction"},{"location":"loss/#usage-example","text":"import torch import numpy as np from few_shot_learning_nlp.loss import FocalLoss # Assuming train_df is your training DataFrame _, class_counts = np.unique(train_df['label'], return_counts=True) # Calculate alpha alpha = len(train_df['label']) / class_counts # Initialize FocalLoss loss_fn = FocalLoss(alpha, gamma=2)","title":"Usage example"},{"location":"loss/#implementation","text":"","title":"Implementation"},{"location":"loss/#initialization","text":"def __init__( self, alpha: Union[list, float, int], gamma: int = 2, device: str = 'cuda' ): alpha : The weight factor(s) for each class to address class imbalance. If a single value is provided, it is assumed to be the weight for the positive class, and the weight for the negative class is calculated as 1 - alpha. If a list is provided, it should contain weight factors for each class. gamma : The focusing parameter to control the degree of adjustment for misclassified samples. Higher values of gamma give more weight to hard-to-classify examples, reducing the influence of easy examples (default: 2). device : The device on which to perform calculations ('cuda' or 'cpu') (default: 'cuda').","title":"Initialization"},{"location":"loss/#attributes","text":"alpha (torch.Tensor): The calculated weight factors for each class. gamma (float): The focusing parameter. device (str): The device on which calculations are performed.","title":"Attributes"},{"location":"loss/#methods","text":"forward(inputs, targets) Compute the Focal Loss given the input predictions and target labels. Args: inputs (torch.Tensor): The input predictions or logits from the model. targets (torch.Tensor): The target class labels. Returns: torch.Tensor : The computed Focal Loss value.","title":"Methods"},{"location":"loss/#note","text":"Ensure that the inputs are logits or unnormalized probabilities, and the targets are class labels. This implementation supports binary or multiclass classification tasks.","title":"Note"},{"location":"pet/","text":"Pattern Exploiting Training (PET) Introduction Pattern Exploiting Training (PET) is a technique used for fine-tuning pretrained language models for text classification tasks. It leverages patterns in the input text to improve classification performance. This page provides an overview of how to use PET for text classification, along with a brief usage example. Usage example Imports import transformers from transformers import AutoTokenizer, AutoModelForMaskedLM from datasets import load_dataset import torch import matplotlib.pyplot as plt import numpy as np from tqdm import tqdm from torcheval.metrics.functional import multiclass_confusion_matrix, binary_f1_score, multiclass_f1_score import seaborn as sns import pandas as pd from torch.utils.data import DataLoader from few_shot_learning_nlp.few_shot_text_classification.pattern_exploiting import PETTrainer from few_shot_learning_nlp.few_shot_text_classification.pattern_exploiting_dataset import PETDatasetForClassification from few_shot_learning_nlp.utils import stratified_train_test_split Preprocessing # Load AG News dataset ag_news_dataset = load_dataset(\"ag_news\") # Define a pattern for PET def pattern1(text: str, tokenizer: AutoTokenizer): return f\"{tokenizer.mask_token} news: {text}\" # Instantiate the tokenizer tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # Get class names and number of classes class_names = ag_news_dataset['train'].features['label'].names num_classes = len(class_names) # Define verbalizer and inverse verbalizer verbalizer = {idx: tokenizer.vocab[x.lower()] for idx, x in enumerate(class_names)} inverse_verbalizer = {tokenizer.vocab[x.lower()]: idx for idx, x in enumerate(class_names)} # Preprocess the data def preprocess(text: List[str], labels: List[int]): processed_text = [] processed_labels = [] for idx in range(len(text)): label = idx2classes[labels[idx]] text_ = text[idx] processed_text.append(pattern1(text_, tokenizer)) processed_labels.append(label) return processed_text, processed_labels train_text, train_labels = preprocess(train_df['text'], train_df['label']) val_text, val_labels = preprocess(val_df['text'], val_df['label']) test_text, test_labels = preprocess(test_df['text'], test_df['label']) # Create datasets and dataloaders train_dataset = PETDatasetForClassification(train_text, train_labels, tokenizer) val_dataset = PETDatasetForClassification(val_text, val_labels, tokenizer) test_dataset = PETDatasetForClassification(test_text, test_labels, tokenizer) train_dataloader = DataLoader(train_dataset, shuffle=True) val_dataloader = DataLoader(val_dataset) test_dataloader = DataLoader(test_dataset) Training the Models # Load pretrained model model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\").to(device) # Instantiate PETTrainer trainer = PETTrainer(model, verbalizer, tokenizer, num_classes=len(classes)) # Train the model history, confusion_matrix, best_model = trainer.train( train_dataloader, val_dataloader, alpha=1e-6, lr=1e-4, device=device ) Evaluating the Model # Plot F1 score over epochs plt.plot(history) plt.scatter(np.argmax(history), np.max(history), label=f\"f1 = {np.max(history)}\", c=\"green\") plt.grid() plt.title(\"F1 score over epochs\") plt.xlabel(\"epoch\") plt.ylabel(\"F1 score\") plt.legend() # Plot confusion matrix df = pd.DataFrame(confusion_matrix.to(\"cpu\").numpy(), index=class_names, columns=class_names) sns.heatmap(df, annot=True, fmt='2g') plt.title(f\"Confusion Matrix Ag news - PET\") # Test the model y_true_test, y_pred_test = trainer.test(test_dataloader) f1 = multiclass_f1_score(y_pred_test, y_true_test, num_classes=len(classes)) This code demonstrates how to perform text classification using Pattern Exploiting Training (PET) with a few-shot learning approach. It includes steps for dataset loading, preprocessing, model training, evaluation, and testing. Make sure to adapt the paths, hyperparameters, and dataset configurations according to your specific setup and requirements. PETTrainer Introduction The PETTrainer class is designed to fine-tune a model with Pattern Exploiting Training (PET) for text classification tasks. It combines a pretrained language model with PET to enhance classification performance. Initialization def __init__( self, model: AutoModelForMaskedLM, verbalizer: Dict, tokenizer: AutoTokenizer, num_classes: int, device: str = 'cuda', ) -> None: model : Pretrained language model to be fine-tuned. verbalizer : Dictionary mapping class labels to corresponding tokens. tokenizer : Tokenizer associated with the model. num_classes : Number of classes in the classification task. device : Device on which calculations are performed (default: 'cuda'). Methods get_y_true(input, inverse_verbalizer, device) Get the true labels from the input data. input : Input data containing the true labels. inverse_verbalizer : Dictionary mapping tokens to class labels. device : Device on which calculations are performed. train(train_dataloader, val_dataloader, alpha, loss_fn, device, lr, n_epochs) Train the PET model. train_dataloader : DataLoader for the training dataset. val_dataloader : DataLoader for the validation dataset. alpha : Weighting factor for balancing MLM and CE losses. loss_fn : Custom loss function for the CE loss (default: None). device : Device on which calculations are performed (default: 'cuda'). lr : Learning rate for optimization (default: 1e-5). n_epochs : Number of training epochs (default: 10). Returns a tuple containing training history, confusion matrix, and the best trained model. test(test_dataloader) Perform inference/testing using the trained model on the provided test data. test_dataloader : DataLoader containing the test data. Returns a tuple containing true labels and predicted labels. Notes PET (Pattern Exploiting Training) is a technique for fine-tuning pretrained language models for text classification tasks. The trainer supports multi-class classification tasks. The model is trained using a combination of Masked Language Model (MLM) loss and Cross-Entropy (CE) loss. PETDatasetForClassification Introduction The PETDatasetForClassification class is a Dataset class designed for Pattern Exploiting Training (PET) in text classification tasks. It preprocesses input texts, tokenizes them, encodes class labels, and prepares the dataset for PET training. Initialization def __init__( self, processed_text: List[str], labels: List[int], tokenizer: AutoTokenizer, device: str = \"cuda\" ) -> None: processed_text : List of processed input texts. labels : List of corresponding class labels. tokenizer : Tokenizer used to tokenize the input texts. device : Device on which calculations are performed (default: \"cuda\"). Attributes tokens : Tokenized input texts. encoded_labels : Encoded labels for PET training. inputs : Dictionary containing tokenized inputs and encoded labels. Keys: 'input_ids', 'attention_mask', 'labels' device : Device on which calculations are performed. Methods __getitem__(index) Retrieves an item from the dataset at the specified index. Returns a dictionary containing tokenized inputs and encoded labels. __len__() Returns the total number of items in the dataset. Notes This dataset class is designed for use with PET (Pattern Exploiting Training) in text classification tasks. Each input text is tokenized using the provided tokenizer and padded/truncated to the maximum length. The class labels are encoded and replaced with mask tokens for PET training. The dataset is prepared for training on the specified device.","title":"Pattern Exploiting"},{"location":"pet/#pattern-exploiting-training-pet","text":"","title":"Pattern Exploiting Training (PET)"},{"location":"pet/#introduction","text":"Pattern Exploiting Training (PET) is a technique used for fine-tuning pretrained language models for text classification tasks. It leverages patterns in the input text to improve classification performance. This page provides an overview of how to use PET for text classification, along with a brief usage example.","title":"Introduction"},{"location":"pet/#usage-example","text":"","title":"Usage example"},{"location":"pet/#imports","text":"import transformers from transformers import AutoTokenizer, AutoModelForMaskedLM from datasets import load_dataset import torch import matplotlib.pyplot as plt import numpy as np from tqdm import tqdm from torcheval.metrics.functional import multiclass_confusion_matrix, binary_f1_score, multiclass_f1_score import seaborn as sns import pandas as pd from torch.utils.data import DataLoader from few_shot_learning_nlp.few_shot_text_classification.pattern_exploiting import PETTrainer from few_shot_learning_nlp.few_shot_text_classification.pattern_exploiting_dataset import PETDatasetForClassification from few_shot_learning_nlp.utils import stratified_train_test_split","title":"Imports"},{"location":"pet/#preprocessing","text":"# Load AG News dataset ag_news_dataset = load_dataset(\"ag_news\") # Define a pattern for PET def pattern1(text: str, tokenizer: AutoTokenizer): return f\"{tokenizer.mask_token} news: {text}\" # Instantiate the tokenizer tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # Get class names and number of classes class_names = ag_news_dataset['train'].features['label'].names num_classes = len(class_names) # Define verbalizer and inverse verbalizer verbalizer = {idx: tokenizer.vocab[x.lower()] for idx, x in enumerate(class_names)} inverse_verbalizer = {tokenizer.vocab[x.lower()]: idx for idx, x in enumerate(class_names)} # Preprocess the data def preprocess(text: List[str], labels: List[int]): processed_text = [] processed_labels = [] for idx in range(len(text)): label = idx2classes[labels[idx]] text_ = text[idx] processed_text.append(pattern1(text_, tokenizer)) processed_labels.append(label) return processed_text, processed_labels train_text, train_labels = preprocess(train_df['text'], train_df['label']) val_text, val_labels = preprocess(val_df['text'], val_df['label']) test_text, test_labels = preprocess(test_df['text'], test_df['label']) # Create datasets and dataloaders train_dataset = PETDatasetForClassification(train_text, train_labels, tokenizer) val_dataset = PETDatasetForClassification(val_text, val_labels, tokenizer) test_dataset = PETDatasetForClassification(test_text, test_labels, tokenizer) train_dataloader = DataLoader(train_dataset, shuffle=True) val_dataloader = DataLoader(val_dataset) test_dataloader = DataLoader(test_dataset)","title":"Preprocessing"},{"location":"pet/#training-the-models","text":"# Load pretrained model model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\").to(device) # Instantiate PETTrainer trainer = PETTrainer(model, verbalizer, tokenizer, num_classes=len(classes)) # Train the model history, confusion_matrix, best_model = trainer.train( train_dataloader, val_dataloader, alpha=1e-6, lr=1e-4, device=device )","title":"Training the Models"},{"location":"pet/#evaluating-the-model","text":"# Plot F1 score over epochs plt.plot(history) plt.scatter(np.argmax(history), np.max(history), label=f\"f1 = {np.max(history)}\", c=\"green\") plt.grid() plt.title(\"F1 score over epochs\") plt.xlabel(\"epoch\") plt.ylabel(\"F1 score\") plt.legend() # Plot confusion matrix df = pd.DataFrame(confusion_matrix.to(\"cpu\").numpy(), index=class_names, columns=class_names) sns.heatmap(df, annot=True, fmt='2g') plt.title(f\"Confusion Matrix Ag news - PET\") # Test the model y_true_test, y_pred_test = trainer.test(test_dataloader) f1 = multiclass_f1_score(y_pred_test, y_true_test, num_classes=len(classes)) This code demonstrates how to perform text classification using Pattern Exploiting Training (PET) with a few-shot learning approach. It includes steps for dataset loading, preprocessing, model training, evaluation, and testing. Make sure to adapt the paths, hyperparameters, and dataset configurations according to your specific setup and requirements.","title":"Evaluating the Model"},{"location":"pet/#pettrainer","text":"","title":"PETTrainer"},{"location":"pet/#introduction_1","text":"The PETTrainer class is designed to fine-tune a model with Pattern Exploiting Training (PET) for text classification tasks. It combines a pretrained language model with PET to enhance classification performance.","title":"Introduction"},{"location":"pet/#initialization","text":"def __init__( self, model: AutoModelForMaskedLM, verbalizer: Dict, tokenizer: AutoTokenizer, num_classes: int, device: str = 'cuda', ) -> None: model : Pretrained language model to be fine-tuned. verbalizer : Dictionary mapping class labels to corresponding tokens. tokenizer : Tokenizer associated with the model. num_classes : Number of classes in the classification task. device : Device on which calculations are performed (default: 'cuda').","title":"Initialization"},{"location":"pet/#methods","text":"get_y_true(input, inverse_verbalizer, device) Get the true labels from the input data. input : Input data containing the true labels. inverse_verbalizer : Dictionary mapping tokens to class labels. device : Device on which calculations are performed. train(train_dataloader, val_dataloader, alpha, loss_fn, device, lr, n_epochs) Train the PET model. train_dataloader : DataLoader for the training dataset. val_dataloader : DataLoader for the validation dataset. alpha : Weighting factor for balancing MLM and CE losses. loss_fn : Custom loss function for the CE loss (default: None). device : Device on which calculations are performed (default: 'cuda'). lr : Learning rate for optimization (default: 1e-5). n_epochs : Number of training epochs (default: 10). Returns a tuple containing training history, confusion matrix, and the best trained model. test(test_dataloader) Perform inference/testing using the trained model on the provided test data. test_dataloader : DataLoader containing the test data. Returns a tuple containing true labels and predicted labels.","title":"Methods"},{"location":"pet/#notes","text":"PET (Pattern Exploiting Training) is a technique for fine-tuning pretrained language models for text classification tasks. The trainer supports multi-class classification tasks. The model is trained using a combination of Masked Language Model (MLM) loss and Cross-Entropy (CE) loss.","title":"Notes"},{"location":"pet/#petdatasetforclassification","text":"","title":"PETDatasetForClassification"},{"location":"pet/#introduction_2","text":"The PETDatasetForClassification class is a Dataset class designed for Pattern Exploiting Training (PET) in text classification tasks. It preprocesses input texts, tokenizes them, encodes class labels, and prepares the dataset for PET training.","title":"Introduction"},{"location":"pet/#initialization_1","text":"def __init__( self, processed_text: List[str], labels: List[int], tokenizer: AutoTokenizer, device: str = \"cuda\" ) -> None: processed_text : List of processed input texts. labels : List of corresponding class labels. tokenizer : Tokenizer used to tokenize the input texts. device : Device on which calculations are performed (default: \"cuda\").","title":"Initialization"},{"location":"pet/#attributes","text":"tokens : Tokenized input texts. encoded_labels : Encoded labels for PET training. inputs : Dictionary containing tokenized inputs and encoded labels. Keys: 'input_ids', 'attention_mask', 'labels' device : Device on which calculations are performed.","title":"Attributes"},{"location":"pet/#methods_1","text":"__getitem__(index) Retrieves an item from the dataset at the specified index. Returns a dictionary containing tokenized inputs and encoded labels. __len__() Returns the total number of items in the dataset.","title":"Methods"},{"location":"pet/#notes_1","text":"This dataset class is designed for use with PET (Pattern Exploiting Training) in text classification tasks. Each input text is tokenized using the provided tokenizer and padded/truncated to the maximum length. The class labels are encoded and replaced with mask tokens for PET training. The dataset is prepared for training on the specified device.","title":"Notes"},{"location":"setfit/","text":"SetFit Introduction SetFit is a framework for few-shot fine-tuning of Sentence Transformers, developed by Hugging Face. Key Features Prompt-Free Approach : Unlike other few-shot learning methods, SetFit does not require handcrafted prompts or verbalisers. It generates rich embeddings directly from a small number of labeled text examples. Efficiency : SetFit achieves high accuracy without the need for large-scale models like T0 or GPT-3, making it significantly faster to train and run inference with. Multilingual Support : SetFit can be used with any Sentence Transformer model available on the Hub, enabling text classification in multiple languages with ease. How It Works SetFit adopts a two-stage training process: 1. Fine-tuning the Sentence Transformer : In the initial stage, SetFit fine-tunes a Sentence Transformer model on a small number of labeled examples using contrastive training. The model learns to generate dense embeddings for each example. 2. Training the Classifier : In the second stage, SetFit trains a classifier head on the embeddings generated by the fine-tuned Sentence Transformer. This classifier can then predict the labels for unseen examples based on their embeddings. Usage example Preprocessing from datasets import load_dataset import pandas as pd from few_shot_learning_nlp.utils import stratified_train_test_split from torch.utils.data import DataLoader from few_shot_learning_nlp.few_shot_text_classification.setfit_dataset import SetFitDataset # Load a dataset for text classification ag_news_dataset = load_dataset(\"ag_news\") # Extract necessary information from the dataset num_classes = len(ag_news_dataset['train'].features['label'].names) # Perform few-shot learning by selecting a limited number of classes n_shots = 50 train_validation, test_df = stratified_train_test_split(ag_news_dataset['train'], num_shots_per_class=n_shots) train_df, val_df = stratified_train_test_split(pd.DataFrame(train_validation), num_shots_per_class=30) # Create SetFitDataset objects for training and validation set_fit_data_train = SetFitDataset(train_df['text'], train_df['label'], input_example_format=True) set_fit_data_val = SetFitDataset(val_df['text'], val_df['label'], input_example_format=False) # Create DataLoader objects for training and validation datasets train_dataloader = DataLoader(set_fit_data_train.data, shuffle=False) val_dataloader = DataLoader(set_fit_data_val) Defining Classifier import torch class CLF(torch.nn.Module): def __init__( self, in_features : int, out_features : int, *args, **kwargs ) -> None: super().__init__(*args, **kwargs) self.layer1 = torch.nn.Linear(in_features, 128) self.relu = torch.nn.ReLU() self.layer2 = torch.nn.Linear(128, 32) self.layer3 = torch.nn.Linear(32, out_features) def forward(self, x : torch.Tensor): x = self.layer1(x) x = self.relu(x) x = self.layer2(x) x = self.relu(x) return self.layer3(x) Training the Embedding Model import torch from sentence_transformers import SentenceTransformer from few_shot_learning_nlp.few_shot_text_classification.setfit import SetFitTrainer # Load a pre-trained Sentence Transformer model model = SentenceTransformer(\"whaleloops/phrase-bert\") # Initialize the SetFitTrainer with embedding model and classifier embedding_model = model.to(\"cuda\") in_features = embedding_model.get_sentence_embedding_dimension() clf = CLF(in_features, num_classes).to(\"cuda\") trainer = SetFitTrainer(embedding_model, clf, num_classes) # Train the embedding model trainer.train_embedding(train_dataloader, val_dataloader, n_epochs=10) Training the Classifier Model # Shuffle training data _, class_counts = np.unique(train_df['label'], return_counts=True) X_train_shuffled, y_train_shuffled = shuffle_two_lists(train_df['text'], train_df['label']) # Train the classifier history, embedding_model, clf = trainer.train_classifier( X_train_shuffled, y_train_shuffled, val_df['text'], val_df['label'], clf=CLF(in_features, num_classes), n_epochs=15, lr=1e-4 ) Testing the Models y_true, y_pred = trainer.test(test_df) SetFitTrainer Introduction The SetFitTrainer class is designed to facilitate the training and testing of embedding and classification models using Sentence Transformers and PyTorch. It provides methods for training embedding models, training classifier models, and testing the performance of trained models on test datasets. Initialization def __init__( self, embedding_model, classifier_model: torch.nn.Module, num_classes: int, dataset_name: str = None, model_name: str = None, device: str ='cuda', ) -> None: embedding_model : Pre-trained embedding model. classifier_model : Classifier model for text classification. num_classes : Number of classes in the classification task. dataset_name : Name of the dataset (optional). model_name : Name of the model (optional). device : Device on which calculations are performed (default: \"cuda\"). Methods train_embedding(train_dataloader, val_dataloader, n_epochs=10, filepath=None, **kwargs) Train the embedding model using the provided training dataloader and validate it using the validation dataloader. Args: train_dataloader : DataLoader containing the training data. val_dataloader : DataLoader containing the validation data. n_epochs : Number of epochs for training (default: 10). filepath : Filepath to save the best model (default: None). **kwargs : Additional keyword arguments to pass to the embedding model's fit method. Returns: None train_classifier(X_train, y_train, X_val, y_val, n_epochs=100, loss_fn=torch.nn.CrossEntropyLoss(), embedding_model=None, clf=None, lr=1e-5) Train the classifier model using the provided training and validation data. Args: X_train : List of training texts. y_train : List of corresponding training labels. X_val : List of validation texts. y_val : List of corresponding validation labels. n_epochs : Number of epochs for training (default: 100). loss_fn : Loss function for training (default: torch.nn.CrossEntropyLoss()). embedding_model : Pre-trained embedding model to use. If None, uses the best_model (default: None). clf : Classifier model to use. If None, uses self.clf (default: None). lr : Learning rate for optimizer (default: 1e-5). Returns: Tuple containing the history of F1 scores during training, the embedding model, and the best classifier model. test(test_df, embedding_model=None, clf=None) Test the performance of the trained models on the provided test dataset. Args: test_df : DataFrame containing the test dataset with 'text' and 'label' columns. embedding_model : SentenceTransformer model for text embedding. If None, the best trained embedding model will be used (default: None). clf : Trained classifier model. If None, the best trained classifier will be used (default: None). Returns: Tuple containing the true labels and predicted labels for the test dataset. SetFitDataset Introduction The SetFitDataset class is designed to create pairs of texts with their corresponding labels for training. It expands the dataset by considering all possible pairs of texts or randomly selecting pairs within a specified radius. Initialization def __init__( self, text: List[str], labels: List[int], R: int = -1, input_example_format: bool = True ) -> None: text : List of texts. labels : List of corresponding labels. R : Radius for data expansion. If negative, considers all possible pairs within the dataset. If positive, randomly selects pairs within the specified radius (default: -1). input_example_format : If True, returns expanded data in the InputExample format. If False, returns expanded data as a list of lists (default: True). Attributes data : Expanded dataset containing pairs of texts with their labels. Methods expand_data(X, y, R, input_example_format) Static method to expand the dataset by creating pairs of texts with their corresponding labels. Args: X : List of texts. y : List of corresponding labels. R : Radius for data expansion. If negative, considers all possible pairs within the dataset. If positive, randomly selects pairs within the specified radius (default: -1). input_example_format : If True, returns expanded data in the InputExample format. If False, returns expanded data as a list of lists (default: True). Returns: Expanded dataset containing pairs of texts with their labels. __len__() Returns the length of the dataset. __getitem__(index) Returns the item at the specified index in the dataset. ```","title":"Setfit"},{"location":"setfit/#setfit","text":"","title":"SetFit"},{"location":"setfit/#introduction","text":"SetFit is a framework for few-shot fine-tuning of Sentence Transformers, developed by Hugging Face.","title":"Introduction"},{"location":"setfit/#key-features","text":"Prompt-Free Approach : Unlike other few-shot learning methods, SetFit does not require handcrafted prompts or verbalisers. It generates rich embeddings directly from a small number of labeled text examples. Efficiency : SetFit achieves high accuracy without the need for large-scale models like T0 or GPT-3, making it significantly faster to train and run inference with. Multilingual Support : SetFit can be used with any Sentence Transformer model available on the Hub, enabling text classification in multiple languages with ease.","title":"Key Features"},{"location":"setfit/#how-it-works","text":"SetFit adopts a two-stage training process: 1. Fine-tuning the Sentence Transformer : In the initial stage, SetFit fine-tunes a Sentence Transformer model on a small number of labeled examples using contrastive training. The model learns to generate dense embeddings for each example. 2. Training the Classifier : In the second stage, SetFit trains a classifier head on the embeddings generated by the fine-tuned Sentence Transformer. This classifier can then predict the labels for unseen examples based on their embeddings.","title":"How It Works"},{"location":"setfit/#usage-example","text":"","title":"Usage example"},{"location":"setfit/#preprocessing","text":"from datasets import load_dataset import pandas as pd from few_shot_learning_nlp.utils import stratified_train_test_split from torch.utils.data import DataLoader from few_shot_learning_nlp.few_shot_text_classification.setfit_dataset import SetFitDataset # Load a dataset for text classification ag_news_dataset = load_dataset(\"ag_news\") # Extract necessary information from the dataset num_classes = len(ag_news_dataset['train'].features['label'].names) # Perform few-shot learning by selecting a limited number of classes n_shots = 50 train_validation, test_df = stratified_train_test_split(ag_news_dataset['train'], num_shots_per_class=n_shots) train_df, val_df = stratified_train_test_split(pd.DataFrame(train_validation), num_shots_per_class=30) # Create SetFitDataset objects for training and validation set_fit_data_train = SetFitDataset(train_df['text'], train_df['label'], input_example_format=True) set_fit_data_val = SetFitDataset(val_df['text'], val_df['label'], input_example_format=False) # Create DataLoader objects for training and validation datasets train_dataloader = DataLoader(set_fit_data_train.data, shuffle=False) val_dataloader = DataLoader(set_fit_data_val)","title":"Preprocessing"},{"location":"setfit/#defining-classifier","text":"import torch class CLF(torch.nn.Module): def __init__( self, in_features : int, out_features : int, *args, **kwargs ) -> None: super().__init__(*args, **kwargs) self.layer1 = torch.nn.Linear(in_features, 128) self.relu = torch.nn.ReLU() self.layer2 = torch.nn.Linear(128, 32) self.layer3 = torch.nn.Linear(32, out_features) def forward(self, x : torch.Tensor): x = self.layer1(x) x = self.relu(x) x = self.layer2(x) x = self.relu(x) return self.layer3(x)","title":"Defining Classifier"},{"location":"setfit/#training-the-embedding-model","text":"import torch from sentence_transformers import SentenceTransformer from few_shot_learning_nlp.few_shot_text_classification.setfit import SetFitTrainer # Load a pre-trained Sentence Transformer model model = SentenceTransformer(\"whaleloops/phrase-bert\") # Initialize the SetFitTrainer with embedding model and classifier embedding_model = model.to(\"cuda\") in_features = embedding_model.get_sentence_embedding_dimension() clf = CLF(in_features, num_classes).to(\"cuda\") trainer = SetFitTrainer(embedding_model, clf, num_classes) # Train the embedding model trainer.train_embedding(train_dataloader, val_dataloader, n_epochs=10)","title":"Training the Embedding Model "},{"location":"setfit/#training-the-classifier-model","text":"# Shuffle training data _, class_counts = np.unique(train_df['label'], return_counts=True) X_train_shuffled, y_train_shuffled = shuffle_two_lists(train_df['text'], train_df['label']) # Train the classifier history, embedding_model, clf = trainer.train_classifier( X_train_shuffled, y_train_shuffled, val_df['text'], val_df['label'], clf=CLF(in_features, num_classes), n_epochs=15, lr=1e-4 )","title":"Training the Classifier Model "},{"location":"setfit/#testing-the-models","text":"y_true, y_pred = trainer.test(test_df)","title":"Testing the Models "},{"location":"setfit/#setfittrainer","text":"","title":"SetFitTrainer"},{"location":"setfit/#introduction_1","text":"The SetFitTrainer class is designed to facilitate the training and testing of embedding and classification models using Sentence Transformers and PyTorch. It provides methods for training embedding models, training classifier models, and testing the performance of trained models on test datasets.","title":"Introduction"},{"location":"setfit/#initialization","text":"def __init__( self, embedding_model, classifier_model: torch.nn.Module, num_classes: int, dataset_name: str = None, model_name: str = None, device: str ='cuda', ) -> None: embedding_model : Pre-trained embedding model. classifier_model : Classifier model for text classification. num_classes : Number of classes in the classification task. dataset_name : Name of the dataset (optional). model_name : Name of the model (optional). device : Device on which calculations are performed (default: \"cuda\").","title":"Initialization"},{"location":"setfit/#methods","text":"train_embedding(train_dataloader, val_dataloader, n_epochs=10, filepath=None, **kwargs) Train the embedding model using the provided training dataloader and validate it using the validation dataloader. Args: train_dataloader : DataLoader containing the training data. val_dataloader : DataLoader containing the validation data. n_epochs : Number of epochs for training (default: 10). filepath : Filepath to save the best model (default: None). **kwargs : Additional keyword arguments to pass to the embedding model's fit method. Returns: None train_classifier(X_train, y_train, X_val, y_val, n_epochs=100, loss_fn=torch.nn.CrossEntropyLoss(), embedding_model=None, clf=None, lr=1e-5) Train the classifier model using the provided training and validation data. Args: X_train : List of training texts. y_train : List of corresponding training labels. X_val : List of validation texts. y_val : List of corresponding validation labels. n_epochs : Number of epochs for training (default: 100). loss_fn : Loss function for training (default: torch.nn.CrossEntropyLoss()). embedding_model : Pre-trained embedding model to use. If None, uses the best_model (default: None). clf : Classifier model to use. If None, uses self.clf (default: None). lr : Learning rate for optimizer (default: 1e-5). Returns: Tuple containing the history of F1 scores during training, the embedding model, and the best classifier model. test(test_df, embedding_model=None, clf=None) Test the performance of the trained models on the provided test dataset. Args: test_df : DataFrame containing the test dataset with 'text' and 'label' columns. embedding_model : SentenceTransformer model for text embedding. If None, the best trained embedding model will be used (default: None). clf : Trained classifier model. If None, the best trained classifier will be used (default: None). Returns: Tuple containing the true labels and predicted labels for the test dataset.","title":"Methods"},{"location":"setfit/#setfitdataset","text":"","title":"SetFitDataset"},{"location":"setfit/#introduction_2","text":"The SetFitDataset class is designed to create pairs of texts with their corresponding labels for training. It expands the dataset by considering all possible pairs of texts or randomly selecting pairs within a specified radius.","title":"Introduction"},{"location":"setfit/#initialization_1","text":"def __init__( self, text: List[str], labels: List[int], R: int = -1, input_example_format: bool = True ) -> None: text : List of texts. labels : List of corresponding labels. R : Radius for data expansion. If negative, considers all possible pairs within the dataset. If positive, randomly selects pairs within the specified radius (default: -1). input_example_format : If True, returns expanded data in the InputExample format. If False, returns expanded data as a list of lists (default: True).","title":"Initialization"},{"location":"setfit/#attributes","text":"data : Expanded dataset containing pairs of texts with their labels.","title":"Attributes"},{"location":"setfit/#methods_1","text":"expand_data(X, y, R, input_example_format) Static method to expand the dataset by creating pairs of texts with their corresponding labels. Args: X : List of texts. y : List of corresponding labels. R : Radius for data expansion. If negative, considers all possible pairs within the dataset. If positive, randomly selects pairs within the specified radius (default: -1). input_example_format : If True, returns expanded data in the InputExample format. If False, returns expanded data as a list of lists (default: True). Returns: Expanded dataset containing pairs of texts with their labels. __len__() Returns the length of the dataset. __getitem__(index) Returns the item at the specified index in the dataset. ```","title":"Methods"},{"location":"train_test_split/","text":"Stratified Train-Test Split This function splits a dataset into training and validation sets while preserving the class distribution. Usage To use this function, follow the example below: import pandas as pd from sklearn.datasets import fetch_20newsgroups # Load dataset newsgroups_data = fetch_20newsgroups(subset='all') df = pd.DataFrame({'text': newsgroups_data.data, 'label': newsgroups_data.target}) classes = np.unique(df['label'].values) # Split dataset train_data, validation_data = stratified_train_test_split(df, classes, train_size=0.8) Parameters dataset (Union[pd.DataFrame, datasets.Dataset]): The input dataset. classes (np.ndarray): The array of unique class labels present in the dataset. train_size (Union[float, int]): The proportion of the dataset to include in the training split. Should be a float in the range (0, 1) if expressed as a fraction, or an integer if expressed as a number of samples. Returns A tuple containing two dictionaries representing the training and validation data splits: Each dictionary contains two keys: 'label' and 'text'. The 'label' key corresponds to a list of class labels. The 'text' key corresponds to a list of text samples. Notes Ensure that the dataset contains columns named 'label' and 'text' representing the class labels and text samples, respectively. The 'label' column should contain categorical class labels. The 'text' column should contain textual data. If the dataset is a pandas DataFrame, it should be in the format where each row represents a sample, and each column represents a feature. import numpy as np from datasets import Dataset import pandas as pd train_data, validation_data = stratified_train_test_split(df, classes, train_size=0.8) ```","title":"Train test split"},{"location":"train_test_split/#stratified-train-test-split","text":"This function splits a dataset into training and validation sets while preserving the class distribution.","title":"Stratified Train-Test Split"},{"location":"train_test_split/#usage","text":"To use this function, follow the example below: import pandas as pd from sklearn.datasets import fetch_20newsgroups # Load dataset newsgroups_data = fetch_20newsgroups(subset='all') df = pd.DataFrame({'text': newsgroups_data.data, 'label': newsgroups_data.target}) classes = np.unique(df['label'].values) # Split dataset train_data, validation_data = stratified_train_test_split(df, classes, train_size=0.8)","title":"Usage"},{"location":"train_test_split/#parameters","text":"dataset (Union[pd.DataFrame, datasets.Dataset]): The input dataset. classes (np.ndarray): The array of unique class labels present in the dataset. train_size (Union[float, int]): The proportion of the dataset to include in the training split. Should be a float in the range (0, 1) if expressed as a fraction, or an integer if expressed as a number of samples.","title":"Parameters"},{"location":"train_test_split/#returns","text":"A tuple containing two dictionaries representing the training and validation data splits: Each dictionary contains two keys: 'label' and 'text'. The 'label' key corresponds to a list of class labels. The 'text' key corresponds to a list of text samples.","title":"Returns"},{"location":"train_test_split/#notes","text":"Ensure that the dataset contains columns named 'label' and 'text' representing the class labels and text samples, respectively. The 'label' column should contain categorical class labels. The 'text' column should contain textual data. If the dataset is a pandas DataFrame, it should be in the format where each row represents a sample, and each column represents a feature. import numpy as np from datasets import Dataset import pandas as pd train_data, validation_data = stratified_train_test_split(df, classes, train_size=0.8) ```","title":"Notes"}]}