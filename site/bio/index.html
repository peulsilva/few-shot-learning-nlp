<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Classbite - Few shot learning NLP</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="https://unpkg.com/katex@0/dist/katex.min.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Classbite";
        var mkdocs_page_input_path = "bio.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Few shot learning NLP
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../setfit/">Setfit</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../pet/">Pattern Exploiting</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Classbite</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#usage-example">Usage example</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#imports">Imports</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#pre-processing">Pre-processing</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#training">Training</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#evaluation">Evaluation</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#title-classbite-class-documentation">title: Classbite Class Documentation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#classbitetrainer">ClassbiteTrainer</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#methods">Methods</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#attributes">Attributes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#generate_dataset-function">generate_dataset Function</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#arguments">Arguments</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#returns">Returns</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#notes">Notes</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../container/">ContaiNER</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../loss/">Focal Loss</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../images/">Image documents transformer</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../train_test_split/">Train test split</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Few shot learning NLP</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Classbite</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="classbite-for-named-entity-recognition-on-image-documents">CLASSBITE for Named Entity Recognition on Image Documents</h1>
<h2 id="introduction">Introduction</h2>
<p>The <a href="https://arxiv.org/abs/2305.04928">CLASSBITE</a> approach adapts multi-class classification into a binary framework by predicting the class membership of each token in a text. It achieves this by appending each token with its possible class label, thereby increasing the data volume by a factor of k, where k represents the number of classes. This augmentation enables binary classification for each token, facilitating more granular classification within the document.</p>
<p>In the CLASSBITE approach, suppose we have the classes C = [City, Transport, None]. For a given sentence "Paris has a good metro system", we transform it into three separate sentences:</p>
<ol>
<li>
<p>City [SEP] Paris has a good metro system</p>
</li>
<li>
<p>Transport [SEP] Paris has a good metro system</p>
</li>
<li>
<p>None [SEP] Paris has a good metro system</p>
</li>
</ol>
<p>Each sentence is treated as a binary classification task to predict whether each token belongs to its corresponding class or not. This approach effectively increases the data volume and allows for more precise classification within the document.</p>
<h2 id="usage-example">Usage example</h2>
<h3 id="imports">Imports</h3>
<pre><code class="language-python">%load_ext autoreload
%autoreload 2

import transformers
from transformers import AutoTokenizer, AutoModelForTokenClassification
from datasets import load_dataset, Dataset
import torch
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from torcheval.metrics.functional import multiclass_f1_score, multiclass_confusion_matrix, binary_f1_score

from few_shot_learning_nlp.few_shot_ner_image_documents.classbite import ClassbiteTrainer
from few_shot_learning_nlp.few_shot_ner_image_documents.image_dataset import ImageLayoutDataset
from few_shot_learning_nlp.few_shot_ner_image_documents.classbite_dataset import generate_dataset

device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
</code></pre>
<h3 id="pre-processing">Pre-processing</h3>
<p>The dataset is loaded from the FUNSD dataset, and necessary pre-processing steps are performed, including tokenization and dataset generation.</p>
<h3 id="training">Training</h3>
<pre><code class="language-python"># Loading FUNSD Dataset
funsd_dataset = load_dataset(&quot;nielsr/funsd&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)

# Generating dataset
train_data = generate_dataset(funsd_dataset['train'], label_names, idx2label, tokenizer, n_shots=2)
val_data = generate_dataset(Dataset.from_dict(funsd_dataset['train'][10:]), label_names, idx2label, tokenizer, n_shots=50)
test_data = generate_dataset(funsd_dataset['test'], label_names, idx2label, tokenizer, n_shots=np.inf)

# Creating DataLoader
train_dataset = ImageLayoutDataset(train_data, tokenizer)
train_dataloader = DataLoader(train_dataset, shuffle=False)

validation_dataset = ImageLayoutDataset(val_data, tokenizer)
validation_dataloader = DataLoader(validation_dataset, shuffle=False, batch_size=4)

test_dataset = ImageLayoutDataset(test_data, tokenizer)
test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=4)

# Initializing and training the model
model = AutoModelForTokenClassification.from_pretrained(&quot;bert-base-uncased&quot;, num_labels=2)
model.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)

trainer = ClassbiteTrainer(model, optimizer, n_classes)

history = trainer.train(train_dataloader, validation_dataloader, n_epochs=100)

# Plotting validation performance
best_f1, best_epoch = np.max(history), np.argmax(history)
plt.plot(history)
plt.scatter([best_epoch], [best_f1], color=&quot;green&quot;, label=f&quot;Best f1 : {round(best_f1,3)}&quot;)
plt.legend()
plt.ylabel(&quot;f1 score&quot;)
plt.xlabel(&quot;epoch&quot;)
plt.title(&quot;Validation performance - FUNSD - 2 shots&quot;)
plt.grid()

</code></pre>
<h3 id="evaluation">Evaluation</h3>
<p>The model's performance is evaluated using the test dataset, and the F1 score and confusion matrix are computed.</p>
<pre><code class="language-python">
# Evaluating on the test set
y_true, y_pred = trainer.test(test_dataloader)
f1 = multiclass_f1_score(y_pred.to(torch.int64), y_true.to(torch.int64), num_classes=n_classes)
</code></pre>
<p>This notebook showcases the implementation of Few-Shot Learning for NER on image documents using transformers and PyTorch.</p>
<hr />
<h2 id="title-classbite-class-documentation">title: Classbite Class Documentation</h2>
<h2 id="classbitetrainer">ClassbiteTrainer</h2>
<h3 id="methods">Methods</h3>
<ol>
<li>
<p><code>__init__(model, optimizer, n_classes, device="cuda")</code></p>
<p>Initialize Classbite with the provided token classification model, optimizer, and other parameters.</p>
<ul>
<li><code>model (AutoModelForTokenClassification)</code>: The token classification model to be trained.</li>
<li><code>optimizer (torch.optim)</code>: The optimizer used for training.</li>
<li><code>n_classes (int)</code>: The number of classes for token classification.</li>
<li><code>device (str, optional)</code>: The device where the model will be trained. Defaults to "cuda".</li>
</ul>
</li>
<li>
<p><code>train(train_dataloader, validation_dataloader, n_epochs=20)</code></p>
<p>Train and validate the token classification model.</p>
<ul>
<li><code>train_dataloader (Dataset)</code>: DataLoader containing the training data. Requires batch size of 1.</li>
<li><code>validation_dataloader (Dataset)</code>: DataLoader containing the validation data. Requires batch size of 1.</li>
<li><code>n_epochs (int, optional)</code>: Number of epochs for training. Defaults to 20.</li>
</ul>
<p>Returns:
- <code>history (list)</code>: History of evaluation metric (F1-score) during training.</p>
</li>
<li>
<p><code>test(test_dataloader)</code></p>
<p>Performs testing on the provided test dataloader.</p>
<ul>
<li><code>test_dataloader (DataLoader)</code>: The dataloader containing the test dataset. Requires batch size of 1.</li>
</ul>
<p>Returns:
- <code>Tuple[torch.Tensor, torch.Tensor]</code>: A tuple containing the true labels and predicted labels.</p>
</li>
</ol>
<h3 id="attributes">Attributes</h3>
<ul>
<li><code>history (list)</code>: List to store the evaluation metric (F1-score) history during training.</li>
<li><code>best_model (AutoModelForTokenClassification)</code>: The best-performing model based on validation F1-score.</li>
<li><code>n_classes (int)</code>: The number of classes for token classification.</li>
<li><code>model (AutoModelForTokenClassification)</code>: The token classification model.</li>
<li><code>optimizer (torch.optim)</code>: The optimizer used for training.</li>
<li><code>device (str)</code>: The device where the model will be trained.</li>
</ul>
<h2 id="generate_dataset-function">generate_dataset Function</h2>
<p>Generates a new dataset by modifying the original dataset based on the given parameters.</p>
<h3 id="arguments">Arguments</h3>
<ul>
<li><code>dataset (Dataset)</code>: The original dataset.</li>
<li><code>label_names (List[str])</code>: A list of label names to generate the dataset.</li>
<li><code>idx2label (Dict[int, str])</code>: A dictionary mapping label indices to label names.</li>
<li><code>tokenizer (AutoTokenizer)</code>: The tokenizer used to tokenize words.</li>
<li><code>n_shots (int)</code>: The number of shots to consider from the original dataset.</li>
</ul>
<h3 id="returns">Returns</h3>
<ul>
<li><code>Dataset</code>: The generated dataset.</li>
</ul>
<h3 id="notes">Notes</h3>
<ul>
<li>The function generates a new dataset by modifying the original dataset. It creates additional samples based on the provided label names and the number of shots specified.</li>
<li>Each document in the original dataset is processed to generate new samples. Only a limited number of shots (<code>n_shots</code>) are considered from the original dataset.</li>
<li>For each label in <code>label_names</code>, the function creates new samples where the tokens belonging to that label are marked with 1 and other tokens with -100 in the <code>ner_tags</code> field.</li>
<li>The <code>words</code> field in the new dataset contains the tokenized words, and the <code>bboxes</code> field contains the corresponding bounding boxes.</li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../pet/" class="btn btn-neutral float-left" title="Pattern Exploiting"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../container/" class="btn btn-neutral float-right" title="ContaiNER">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../pet/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../container/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../javascripts/katex.js"></script>
      <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      <script src="../javascripts/mathjax.js"></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
